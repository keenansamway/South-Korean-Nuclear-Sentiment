{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "DATA_DIR = Path.cwd() / 'content'\n",
    "MODEL_DIR = Path.cwd() / 'models'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
    "                \n",
    "        self.lstm = nn.LSTM(\n",
    "            config.embedding_dim,\n",
    "            config.rnn_size,\n",
    "            num_layers=config.rnn_num_layers,\n",
    "            # bidirectional=config.bidirectional,\n",
    "            batch_first=config.batch_first,\n",
    "            dropout=config.dropout,\n",
    "        )\n",
    "        \n",
    "        # if config.bidirectional:\n",
    "        #     self.fc = nn.Linear(config.rnn_size * 2, config.out_size)\n",
    "        # else:\n",
    "        self.fc = nn.Linear(config.rnn_size, config.out_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        logits, hidden = self.lstm(embedded)\n",
    "        logits = self.dropout(logits)\n",
    "        logits = logits[:,-1,:]\n",
    "        out = self.fc(logits)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model_to_save, filename='checkpoint.pt'):\n",
    "    torch.save(model_to_save.state_dict(), MODEL_DIR / filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.text = self.df['text'].to_numpy()\n",
    "        self.targets = self.df['pro_nuclear'].to_numpy()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.text[index], self.targets[index]\n",
    "    \n",
    "class MyCollate:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        texts = [item[0] for item in batch]\n",
    "        targets = [item[1] for item in batch]\n",
    "        \n",
    "        texts = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            # truncation=True,\n",
    "            # max_length=128,\n",
    "            return_tensors='pt',\n",
    "        ).input_ids\n",
    "        \n",
    "        targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        \n",
    "        return texts, targets\n",
    "    \n",
    "def get_loader(df, tokenizer, batch_size=32, suffle=True, num_workers=0, pin_memory=True):\n",
    "    \n",
    "    dataset = TextDataset(df)\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=suffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(tokenizer=tokenizer),\n",
    "    )\n",
    "    \n",
    "    return loader, dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(DATA_DIR / 'nuclear_energy_en' / 'processed_data.csv')\n",
    "data_df[\"pro_nuclear\"] = data_df[\"pro_nuclear\"].astype(int)\n",
    "# print(f\"Removing {data_df['text'].isna().sum()} rows with NaN values\")\n",
    "data_df = data_df.dropna()\n",
    "# Remove rows that begin with \"##\"\n",
    "data_df = data_df[~data_df['text'].str.startswith('##')]\n",
    "\n",
    "\n",
    "test_split = 0.2\n",
    "val_split = 0.2\n",
    "\n",
    "train_df, test_df = train_test_split(data_df, test_size=test_split, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=val_split/(1-test_split), random_state=42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', use_fast=False)\n",
    "\n",
    "# print(len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-3\n",
    "dropout = 0.65\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "\n",
    "num_epochs = 20\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, train_dataset = get_loader(train_df, tokenizer=tokenizer, batch_size=batch_size, num_workers=num_workers)\n",
    "val_loader, val_dataset = get_loader(val_df, tokenizer=tokenizer, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader, test_dataset = get_loader(test_df, tokenizer=tokenizer, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.embedding_dim = 768\n",
    "        self.rnn_size = 128\n",
    "        self.rnn_num_layers = 2\n",
    "        self.bidirectional = False\n",
    "        self.batch_first = True\n",
    "        self.dropout = dropout\n",
    "        self.out_size = 1\n",
    "        \n",
    "config = Config(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(config).to(device)\n",
    "# criterion = nn.CrossEntropyLoss(reduction=\"mean\").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"mean\").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 0.7007 - Train Acc: 0.4457 - Val Loss: 0.7001 - Val Acc: 0.4844: 100%|██████████| 8/8 [00:00<00:00, 141.24it/s]\n",
      "Epoch 2/20 - Train Loss: 0.6976 - Train Acc: 0.5217 - Val Loss: 0.7312 - Val Acc: 0.4375: 100%|██████████| 8/8 [00:00<00:00, 133.58it/s]\n",
      "Epoch 3/20 - Train Loss: 0.7003 - Train Acc: 0.5380 - Val Loss: 0.6653 - Val Acc: 0.5625: 100%|██████████| 8/8 [00:00<00:00, 141.15it/s]\n",
      "Epoch 4/20 - Train Loss: 0.6323 - Train Acc: 0.5761 - Val Loss: 0.6533 - Val Acc: 0.5156: 100%|██████████| 8/8 [00:00<00:00, 140.93it/s]\n",
      "Epoch 5/20 - Train Loss: 0.5756 - Train Acc: 0.6685 - Val Loss: 0.5932 - Val Acc: 0.7031: 100%|██████████| 8/8 [00:00<00:00, 138.79it/s]\n",
      "Epoch 6/20 - Train Loss: 0.5621 - Train Acc: 0.7337 - Val Loss: 0.6401 - Val Acc: 0.5781: 100%|██████████| 8/8 [00:00<00:00, 139.63it/s]\n",
      "Epoch 7/20 - Train Loss: 0.5684 - Train Acc: 0.6793 - Val Loss: 0.5915 - Val Acc: 0.7656: 100%|██████████| 8/8 [00:00<00:00, 118.23it/s]\n",
      "Epoch 8/20 - Train Loss: 0.3733 - Train Acc: 0.8696 - Val Loss: 0.6172 - Val Acc: 0.7500: 100%|██████████| 8/8 [00:00<00:00, 141.42it/s]\n",
      "Epoch 9/20 - Train Loss: 0.2583 - Train Acc: 0.9185 - Val Loss: 0.6964 - Val Acc: 0.7500: 100%|██████████| 8/8 [00:00<00:00, 129.54it/s]\n",
      "Epoch 10/20 - Train Loss: 0.1749 - Train Acc: 0.9620 - Val Loss: 0.6850 - Val Acc: 0.7656: 100%|██████████| 8/8 [00:00<00:00, 139.97it/s]\n",
      "Epoch 11/20 - Train Loss: 0.1185 - Train Acc: 0.9728 - Val Loss: 0.8812 - Val Acc: 0.7188: 100%|██████████| 8/8 [00:00<00:00, 135.58it/s]\n",
      "Epoch 12/20 - Train Loss: 0.1002 - Train Acc: 0.9728 - Val Loss: 0.9825 - Val Acc: 0.7500: 100%|██████████| 8/8 [00:00<00:00, 115.66it/s]\n",
      "Epoch 13/20 - Train Loss: 0.0867 - Train Acc: 0.9783 - Val Loss: 0.7754 - Val Acc: 0.7812: 100%|██████████| 8/8 [00:00<00:00, 134.52it/s]\n",
      "Epoch 14/20 - Train Loss: 0.1219 - Train Acc: 0.9728 - Val Loss: 1.1402 - Val Acc: 0.7344: 100%|██████████| 8/8 [00:00<00:00, 136.04it/s]\n",
      "Epoch 15/20 - Train Loss: 0.0501 - Train Acc: 0.9891 - Val Loss: 0.8670 - Val Acc: 0.7656: 100%|██████████| 8/8 [00:00<00:00, 134.76it/s]\n",
      "Epoch 16/20 - Train Loss: 0.0372 - Train Acc: 0.9946 - Val Loss: 0.9865 - Val Acc: 0.7656: 100%|██████████| 8/8 [00:00<00:00, 142.05it/s]\n",
      "Epoch 17/20 - Train Loss: 0.0296 - Train Acc: 0.9946 - Val Loss: 1.0352 - Val Acc: 0.7656: 100%|██████████| 8/8 [00:00<00:00, 93.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss has not decreased in 10 epochs. Stopping training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_loss = None\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ## TRAINING LOOP\n",
    "    train_count = 0\n",
    "    train_loss = 0\n",
    "    avg_train_loss = 0\n",
    "    train_correct = 0\n",
    "    avg_train_acc = 0\n",
    "    pbar_train = tqdm(train_loader, total=len(train_loader), leave=False)\n",
    "    model.train()\n",
    "    for idx, (text, targets) in enumerate(pbar_train):\n",
    "        # Move data to device\n",
    "        \n",
    "        text = text.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(text)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate loss\n",
    "        train_count += 1\n",
    "        train_loss += loss.item()\n",
    "        avg_train_loss = train_loss/train_count\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = torch.round(torch.sigmoid(outputs))\n",
    "        train_correct += (predictions == targets).sum().item()\n",
    "        avg_train_acc = train_correct/(train_count * batch_size)\n",
    "        \n",
    "        # Update progress bar\n",
    "        desc = (\n",
    "            f\"Epoch {epoch+1}/{num_epochs}\"\n",
    "            f\" - Train Loss: {avg_train_loss:.4f}\"\n",
    "            f\" - Train Acc: {avg_train_acc:.4f}\"\n",
    "        )\n",
    "        pbar_train.set_description(desc=desc)\n",
    "        \n",
    "        \n",
    "    ## VALIDATION LOOP   \n",
    "    val_count = 0\n",
    "    val_loss = 0\n",
    "    avg_val_loss = 0\n",
    "    val_correct = 0\n",
    "    avg_val_acc = 0\n",
    "    pbar_val = tqdm(val_loader, total=len(val_loader), leave=True)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (text, targets) in enumerate(pbar_val):\n",
    "            # Move data to device\n",
    "            text = text.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(text)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Calculate loss\n",
    "            val_count += 1\n",
    "            val_loss += loss.item()\n",
    "            avg_val_loss = val_loss/val_count\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = torch.round(torch.sigmoid(outputs))\n",
    "            val_correct += (predictions == targets).sum().item()\n",
    "            avg_val_acc = val_correct/(val_count * batch_size)\n",
    "            \n",
    "            # Update progress bar\n",
    "            desc = (\n",
    "                f\"Epoch {epoch+1}/{num_epochs}\"\n",
    "                f\" - Train Loss: {avg_train_loss:.4f}\"\n",
    "                f\" - Train Acc: {avg_train_acc:.4f}\"\n",
    "                f\" - Val Loss: {avg_val_loss:.4f}\"\n",
    "                f\" - Val Acc: {avg_val_acc:.4f}\"\n",
    "            )\n",
    "            pbar_val.set_description(desc=desc)\n",
    "        \n",
    "        \n",
    "    ## CHECKPOINTING AND EARLY STOPPING\n",
    "    if best_loss is None:   # i.e. first epoch\n",
    "        best_loss = avg_val_loss\n",
    "        save_checkpoint(model, filename=f'checkpoint.pt')\n",
    "        \n",
    "    elif avg_val_loss > best_loss:  # i.e. loss increased\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Validation loss has not decreased in {patience} epochs. Stopping training.\")\n",
    "            break\n",
    "        \n",
    "    else:   # avg_val_loss < best_loss i.e. loss decreased\n",
    "        best_loss = avg_val_loss\n",
    "        save_checkpoint(model, filename=f'checkpoint.pt')\n",
    "        counter = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(28996, 768)\n",
       "  (lstm): LSTM(768, 128, num_layers=2, batch_first=True, dropout=0.65)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.65, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load model\n",
    "prediction_model = RNN(config).to(device)\n",
    "prediction_model.load_state_dict(torch.load(MODEL_DIR / 'checkpoint.pt'))\n",
    "prediction_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text):\n",
    "    tokenized_text = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    out = model(tokenized_text)\n",
    "\n",
    "    pred = torch.sigmoid(out)\n",
    "    rounded_pred = torch.round(pred)\n",
    "\n",
    "    return pred, rounded_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.9334 - Pro Nuclear: 1.0\n"
     ]
    }
   ],
   "source": [
    "txt = \"Nuclear energy is necessary to combat climate change.\"\n",
    "\n",
    "pred, rounded_pred = predict(prediction_model, txt)\n",
    "\n",
    "print(f\"Prediction: {pred.item():.4f} - Pro Nuclear: {rounded_pred.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.0823 - Pro Nuclear: 0.0\n"
     ]
    }
   ],
   "source": [
    "txt = \"Nuclear waste is a huge problem.\"\n",
    "\n",
    "pred, rounded_pred = predict(prediction_model, txt)\n",
    "\n",
    "print(f\"Prediction: {pred.item():.4f} - Pro Nuclear: {rounded_pred.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sk-nuclear",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

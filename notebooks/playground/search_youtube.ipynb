{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- https://developers.google.com/youtube/v3/docs\n",
    "- https://www.thepythoncode.com/article/using-youtube-api-in-python\n",
    "- https://medium.com/daily-python/python-script-to-search-content-using-youtube-data-api-daily-python-8-1084776a6578\n",
    "- https://medium.com/mcd-unison/youtube-data-api-v3-in-python-tutorial-with-examples-e829a25d2ebd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse as p\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pathlib as Path\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "from config import YOUTUBE_API_KEY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YouTube API Info\n",
    "Costs:\n",
    "- Search: 100 credits per request (1-50 videos)\n",
    "- Video Details: 1 credit per request (1 video)\n",
    "- Comment Threads: 1 credit per request (1-100 comment threads)\n",
    "\n",
    "Quota: 10,000 credits per day\n",
    "- Search: max 5,000 videos per day\n",
    "- Video Details: max 10,000 videos per day\n",
    "- Comment Threads: max 1,000,000 comment threads per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_SERVICE_NAME = \"youtube\"\n",
    "API_VERSION = \"v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = build(serviceName=API_SERVICE_NAME, version=API_VERSION, developerKey=YOUTUBE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import isodate\n",
    "\n",
    "def print_video_infos(video_response):\n",
    "    items = video_response.get(\"items\")[0]\n",
    "    # get the snippet, statistics & content details from the video response\n",
    "    snippet         = items[\"snippet\"]\n",
    "    statistics      = items[\"statistics\"]\n",
    "    content_details = items[\"contentDetails\"]\n",
    "    video_id        = items[\"id\"]\n",
    "    # get infos from the snippet\n",
    "    channel_title = snippet[\"channelTitle\"]\n",
    "    title         = snippet[\"title\"]\n",
    "    description   = snippet[\"description\"]\n",
    "    publish_time  = snippet[\"publishedAt\"]\n",
    "    # get stats infos\n",
    "    comment_count = statistics[\"commentCount\"] if \"commentCount\" in statistics else \"NaN\"\n",
    "    like_count    = statistics[\"likeCount\"] if \"likeCount\" in statistics else \"NaN\"\n",
    "    view_count    = statistics[\"viewCount\"]\n",
    "    # get duration from content details\n",
    "    duration = content_details[\"duration\"]\n",
    "    # duration in the form of something like 'PT5H50M15S'\n",
    "    # parsing it to be something like '5:50:15'\n",
    "    duration_str = isodate.strftime(isodate.parse_duration(duration), \"%H:%M:%S\")\n",
    "    \n",
    "    print(f\"\"\"\\\n",
    "    Title: {title}\n",
    "    Channel Title: {channel_title}\n",
    "    Video ID: {video_id}\n",
    "    Publish time: {publish_time}\n",
    "    Duration: {duration_str}\n",
    "    Number of comments: {comment_count}\n",
    "    Number of likes: {like_count}\n",
    "    Number of views: {view_count}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Description: {description}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 API Credits Per Request/Search\n",
    "# https://developers.google.com/youtube/v3/docs/search/list\n",
    "def search(youtube, **kwargs):\n",
    "    return youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 API Credit Per Request/Video\n",
    "# https://developers.google.com/youtube/v3/docs/videos/list\n",
    "def get_video_details(youtube, **kwargs):\n",
    "    return youtube.videos().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        **kwargs,\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 API Credit Per Request/Comment Thread\n",
    "# https://developers.google.com/youtube/v3/docs/commentThreads/list\n",
    "def get_video_comment_threads(youtube, get_replies=False, **kwargs):\n",
    "    part = \"snippet\"\n",
    "    part += \",replies\" if get_replies else \"\"\n",
    "    return youtube.commentThreads().list(\n",
    "        part=part,\n",
    "        **kwargs\n",
    "        ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for the query 'nuclear' and retrieve 2 items only\n",
    "# https://developers.google.com/youtube/v3/docs/search/list\n",
    "response = search(\n",
    "    youtube,\n",
    "    q=\"StopTheSteal\",\n",
    "    pageToken=None, \n",
    "    publishedAfter=\"2021-01-01T00:00:00Z\",\n",
    "    publishedBefore=\"2021-01-12T00:00:00Z\",\n",
    "    order=\"viewCount\", # date, rating, relevance (default), title, videoCount, viewCount\n",
    "    type=\"video\", # channel, playlist, video\n",
    "    maxResults=50, # 0-50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = response.get(\"items\")\n",
    "nextPageToken = response.get(\"nextPageToken\") if \"nextPageToken\" in response else None\n",
    "video_ids = [item[\"id\"][\"videoId\"] for item in items]\n",
    "video_titles = [item[\"snippet\"][\"title\"] for item in items]\n",
    "channel_ids = [item[\"snippet\"][\"channelId\"] for item in items]\n",
    "channel_titles = [item[\"snippet\"][\"channelTitle\"] for item in items]\n",
    "published_time = [item[\"snippet\"][\"publishedAt\"] for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['video_ids'] = video_ids\n",
    "df['video_titles'] = video_titles\n",
    "df['channel_ids'] = channel_ids\n",
    "df['channel_titles'] = channel_titles\n",
    "df['published_time'] = published_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_results(youtube, query, start_date, end_date, results_to_get=50):\n",
    "    \"\"\"\n",
    "    Search for videos based on a query term.\n",
    "    API Cost: 100 credits for 50 search results\n",
    "    Documentation: https://developers.google.com/youtube/v3/docs/search/list\n",
    "    \n",
    "    Parameters:\n",
    "        youtube (object): The YouTube API object.\n",
    "        query (str): The search query term.\n",
    "        start_date (str): The start date for the search query.\n",
    "        end_date (str): The end date for the search query.\n",
    "        max_results (int): The maximum number of results to return.\n",
    "    \n",
    "    Returns:\n",
    "        A pandas DataFrame containing the search results.\n",
    "    \"\"\"\n",
    "    nextPageToken = None\n",
    "    first_page = True    \n",
    "    \n",
    "    search_df = pd.DataFrame()\n",
    "    while (nextPageToken is not None) or (first_page is True):\n",
    "        first_page = False\n",
    "        to_get = 0\n",
    "        if results_to_get >= 50:\n",
    "            to_get = 50\n",
    "            results_to_get -= 50\n",
    "        elif 0 < results_to_get < 50:\n",
    "            to_get = results_to_get\n",
    "            results_to_get = 0\n",
    "        else: # results_to_get <= 0\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            response = youtube.search().list(\n",
    "                part=\"snippet\",\n",
    "                q=query,\n",
    "                pageToken=None,\n",
    "                type=\"video\",\n",
    "                order=\"viewCount\", # date, rating, relevance (default), title, videoCount, viewCount\n",
    "                publishedAfter=start_date + \"T00:00:00Z\",\n",
    "                publishedBefore=end_date + \"T00:00:00Z\",\n",
    "                maxResults=to_get,\n",
    "            ).execute()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "            \n",
    "        nextPageToken = response.get(\"nextPageToken\") if \"nextPageToken\" in response else None\n",
    "\n",
    "        items = response.get(\"items\")\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "        df['video_ids'] = [item[\"id\"][\"videoId\"] for item in items]\n",
    "        df['video_titles'] = [item[\"snippet\"][\"title\"] for item in items]\n",
    "        df['channel_ids'] = [item[\"snippet\"][\"channelId\"] for item in items]\n",
    "        df['channel_titles'] = [item[\"snippet\"][\"channelTitle\"] for item in items]\n",
    "        df['published_time'] = [item[\"snippet\"][\"publishedAt\"] for item in items]\n",
    "    \n",
    "        search_df = pd.concat([search_df, df], ignore_index=True)\n",
    "    \n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_search_results(youtube, \"StopTheSteal\", \"2021-01-01\", \"2021-01-12\", 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_responses = []\n",
    "# for video_id in video_ids:\n",
    "#     # get the video details\n",
    "#     video_response = get_video_details(youtube, id=video_id)\n",
    "#     video_responses.append(video_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 75 comments from page 0\n"
     ]
    }
   ],
   "source": [
    "comments = []\n",
    "video_id = video_ids[1]\n",
    "\n",
    "next_page_token = None\n",
    "first_page = True\n",
    "page_num = 0\n",
    "max_pages = 5\n",
    "\n",
    "while (next_page_token is not None) or (first_page is True):\n",
    "    first_page = False\n",
    "    if page_num >= max_pages:\n",
    "        break\n",
    "    \n",
    "    threads = get_video_comment_threads(\n",
    "        youtube,\n",
    "        videoId=video_id,\n",
    "        get_replies=True,\n",
    "        pageToken=next_page_token,\n",
    "        order=\"time\", # time, relevance (sorting by relevance uses an algorithm which can filter out some comments)\n",
    "        maxResults=100,\n",
    "        )\n",
    "    \n",
    "    next_page_token = threads.get(\"nextPageToken\") if \"nextPageToken\" in threads else None\n",
    "    \n",
    "    comments += threads['items']\n",
    "    \n",
    "    print(f\"Retrieved {len(threads['items'])} comments from page {page_num}\")\n",
    "    page_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_comments = []\n",
    "for comment in comments:\n",
    "    flattened_comments += [comment[\"snippet\"][\"topLevelComment\"]]\n",
    "    if \"replies\" in comment:\n",
    "        flattened_comments += comment[\"replies\"][\"comments\"]\n",
    "\n",
    "flattened_df = pd.json_normalize(flattened_comments, sep=\".\")\n",
    "\n",
    "flattened_df = flattened_df.rename(\n",
    "    columns={\n",
    "        \"snippet.videoId\" : \"videoId\",\n",
    "        \"snippet.textOriginal\" : \"textOriginal\",\n",
    "        \"snippet.authorDisplayName\" : \"authorDisplayName\",\n",
    "        \"snippet.authorChannelId.value\" : \"authorChannelId\",\n",
    "        \"snippet.likeCount\" : \"likeCount\",\n",
    "        \"snippet.publishedAt\" : \"publishedAt\",\n",
    "        \"snippet.updatedAt\" : \"updatedAt\",\n",
    "        \"snippet.parentId\" : \"parentId\",\n",
    "    }\n",
    ")\n",
    "\n",
    "flattened_df = flattened_df[\n",
    "    [\"id\",\n",
    "    \"videoId\",\n",
    "    \"textOriginal\",\n",
    "    \"authorDisplayName\",\n",
    "    \"authorChannelId\",\n",
    "    \"likeCount\",\n",
    "    \"publishedAt\",\n",
    "    \"updatedAt\",\n",
    "    \"parentId\"]\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sk-nuclear",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3120d9f87eabf5535f44bfda2bf17334ee7ee4298f68d473c9683e2ad61a73ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
